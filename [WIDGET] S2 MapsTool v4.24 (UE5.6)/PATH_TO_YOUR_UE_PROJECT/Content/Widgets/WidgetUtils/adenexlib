import csv
import hashlib
import json
import os
import pathlib
import random
import re
import shutil
import string
import subprocess
import tempfile
from functools import wraps
from time import time
from inspect import currentframe
from typing import Callable, Dict, Any, Iterable, Optional, List, Tuple, Literal, Iterator

import unreal

# ============================== Globals ==================================
# Execution/diagnostics state
_execution_times: Dict[str, Dict[str, Any]] = {}
_error_set = set()
_log_set = set()
_asset_cache: Dict[str, Any] = {}
_verbose_logging: bool = True
_timed_logs_enabled: bool = True

# Unreal singletons
editor_asset_lib = unreal.get_editor_subsystem(unreal.EditorAssetSubsystem)
level_lib = unreal.get_editor_subsystem(unreal.EditorActorSubsystem)
asset_tools = unreal.AssetToolsHelpers.get_asset_tools()
u_path = unreal.Paths


# ========================= Logging & Verbosity ============================

def log(text: Any, *, error: bool = False, verbosity_level: str = "INFO") -> None:
    """Structured logging used across the module.

    - Prefixes messages with the *caller* line number for quick traceability.
    - Honors `_verbose_logging` for non-error messages.
    - Collects unique error texts in `_error_set`.
    """
    cf = currentframe().f_back.f_lineno
    formatted_text = f"{cf} - {verbosity_level} - {text}"

    if error:
        unreal.log_error(formatted_text)
        _error_set.add(text)
        return

    else:
        _log_set.add(text)

        if _verbose_logging:
            # unreal.log_warning used to keep messages visible without being errors
            unreal.log_warning(formatted_text)


def show_unreal_message(caption: str, text: str, should_break: bool = False) -> None:
    """Wrapper around `show_message` with SystemExit."""
    unreal.EditorDialog.show_message(unreal.Text(caption), unreal.Text(text), unreal.AppMsgType.OK, unreal.AppReturnType.OK)
    if should_break:
        raise SystemExit


def set_verbose_logging(enabled: bool) -> None:
    """Enable/disable non-error logs (warnings/info)."""
    global _verbose_logging
    _verbose_logging = bool(enabled)


def get_errors() -> set[str]:
    """Return a copy of the collected error messages."""
    return set(_error_set)


def get_logs() -> set[str]:
    """Return a copy of the collected logging messages."""
    return set(_log_set)


def reset_errors() -> None:
    """Clear the collected error messages."""
    _error_set.clear()


def reset_logs() -> None:
    """Clear the collected logging messages."""
    _log_set.clear()


# ========================= Timing & Profiling =============================

def timed(func: Callable) -> Callable:
    """
        Decorator to time a function and log start/finish.
        - Uses perf_counter() for high-resolution timing.
        - Aggregates totals in _execution_times.
    """

    @wraps(func)
    def _wrapper(*args, **kwargs):
        if _timed_logs_enabled:
            log(f"================= '{func.__name__}' started.")
        t0 = time()
        result = func(*args, **kwargs)
        elapsed = time() - t0
        if _timed_logs_enabled:
            log(f"================= '{func.__name__}' finished. Elapsed: {elapsed:.4f} seconds.")
        if func.__name__ in _execution_times:
            _execution_times[func.__name__]['total_time'] += elapsed
            _execution_times[func.__name__]['count'] += 1
        else:
            _execution_times[func.__name__] = {'total_time': elapsed, 'count': 1}
        return result

    return _wrapper


def enable_timed_logging(enabled: bool):
    """Enable/disable timed logs globally."""
    global _timed_logs_enabled
    _timed_logs_enabled = bool(enabled)


def is_verbose_logging_enabled() -> bool:
    """Return the current state of verbose logging."""
    return _verbose_logging


def log_execution_times() -> None:
    """Log total execution times and counts for each timed function."""
    if not _execution_times:
        log("No execution stats recorded yet.")
        return

    log(f"================= Execution stats =================")
    for ex in _execution_times:
        log(f"Function '{ex}' executed {_execution_times[ex]['count']} times, total execution time: {_execution_times[ex]['total_time']:.4f} seconds.")


def reset_execution_stats() -> None:
    """Clear recorded execution stats."""
    _execution_times.clear()


# ============================= Normalization Helpers =================================

def num(s: Any) -> Any:
    """
    Try to convert a string to int or float,
    otherwise return the original value unchanged.
    """
    if isinstance(s, str):
        try:
            s = int(s)
        except ValueError:
            try:
                s = float(s)
            except ValueError:
                return s
    return s


def as_bool(x: Any) -> bool:
    """Convert anything to bool."""
    _bool_true = {"1", "true", "yes", "on"}
    _bool_false = {"0", "false", "no", "off"}

    if isinstance(x, bool):
        return x
    if isinstance(x, (int, float)):
        return bool(x)
    if isinstance(x, str):
        s = x.strip().lower()
        if s in _bool_true:
            return True
        if s in _bool_false:
            return False
    return bool(x)


def as_float(x: Any) -> float:
    """Convert anything to float."""
    if isinstance(x, (int, float)):
        return float(x)
    if isinstance(x, str):
        return float(x.strip())
    raise TypeError(f"Cannot convert {x!r} to float")


def seq_to_rgba(seq: Iterable[Any]) -> Dict[str, float]:
    """Convert a sequence of 4 numbers to RGBA dict."""
    r, g, b, a = seq  # raises if length != 4
    return {"R": float(r), "G": float(g), "B": float(b), "A": float(a)}


def as_rgba(x: Any) -> Dict[str, float]:
    """
    Accepts:
      - list/tuple of 4 numbers
      - string like "[1 1 1 0]" or "[1.0 1.0 1.0 0.0]"
    """
    if isinstance(x, (list, tuple)):
        return seq_to_rgba(x)
    if isinstance(x, str):
        m = re.compile(r'^\s*\[([^\]]+)\]\s*$').match(x)
        if m:
            parts = m.group(1).split()
            if len(parts) == 4:
                return seq_to_rgba(float(p) for p in parts)
    raise TypeError(f"Cannot convert {x!r} to RGBA")


def as_vec(x: Any) -> List[Any]:
    """Convert anything to vec."""
    if isinstance(x, (list, tuple)):
        return x
    if isinstance(x, str):
        m = re.compile(r'^\s*\[([^\]]+)\]\s*$').match(x)
        if m:
            parts = m.group(1).split()
            return [float(p) for p in parts]
    raise TypeError(f"Cannot convert {x!r} to VEC")


# ============================= Data Utils =================================

def clamp(n: float, min_: float, max_: float) -> float:
    """Clamp a number between `min_` and `max_`."""
    return max(min(n, max_), min_)


def random_hex(k: int = 6) -> str:
    """Return a random lowercase hex string of length `k`."""
    return "".join(random.choices("0123456789abcdef", k=k))


def check_ascii(s: str) -> bool:
    """Returns True if all characters are ASCII letters/digits/punct or space."""
    return all(c in string.ascii_letters or c in string.digits or c in string.punctuation or c == ' ' for c in s)


def to_valid_filename(original: str) -> str:
    """Convert arbitrary text to a filesystem-safe name by replacing
        disallowed characters with underscores.
    """
    return re.sub(r'[\s.!@#$%^&*(){}\[\]:;"\'<>,?]', '_', original)


def replace_chars(word: str, chars: str, replacement: str) -> str:
    """Replaces all occurrences of characters in 'chars' within 'word' with the specified 'replacement' character."""
    translation_table = str.maketrans(chars, replacement * len(chars))
    return word.translate(translation_table)


def srgb_to_linear(srgb: float, gamma: float = 2.2) -> float:
    return srgb / 12.92 if srgb <= 0.04045 else ((srgb + 0.055) / 1.055) ** gamma


# ============================ Path & Files =================================

def join_paths(*parts: str) -> str:
    """`os.path.join` + `normpath` convenience."""
    return os.path.normpath(os.path.join(*parts))


def save_json(filepath: str, json_data: Any, indent: int = 4) -> None:
    """Serialize JSON to disk with indentation."""
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=indent)
    log(f"[JSON] File wrote to {filepath}")


def save_text(filepath: str, txt_data: Any) -> None:
    """Write text (string/list/dict) to disk."""
    if isinstance(txt_data, list):
        txt_data = "\n".join(txt_data)
    elif isinstance(txt_data, dict):
        txt_data = str(txt_data)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(txt_data)
    log(f"[TXT] File wrote to {filepath}")


def load_json_file(filepath: str) -> Any:
    """Load and parse a JSON file."""
    try:
        with open(filepath, encoding="utf-8") as json_file:
            return json.loads(json_file.read())
    except Exception as e:  # noqa: BLE001
        log(f"[JSON] Error loading JSON from {filepath}: {str(e)}", error=True)
        return None


def log_pretty_json(json_data: Any) -> None:
    """Pretty-print JSON via the logger."""
    formatted_json = json.dumps(json_data, indent=4, separators=(",", ": "))
    log(formatted_json)


# =========================== CSV & Tabular IO ==============================

def parse_csv(filepath: Optional[str], expected_columns: Iterable[str], row_processor: Callable[[Dict[str, Any]], Any], ) -> list:
    """
    Generic CSV parser that processes rows using a provided function.
    Returns a list of items returned by `row_processor`.
    """
    if not filepath:
        log(f"[CSV] {os.path.basename(filepath)} path not set. Skipping event parsing.", verbosity_level="WARNING")
        return []

    if not os.path.exists(filepath):
        log(f"[CSV] {os.path.basename(filepath)} not found at: {filepath}. Skipping event parsing.", verbosity_level="WARNING")
        return []

    out = []
    try:
        with open(filepath, "r", newline="", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            fields = reader.fieldnames or []

            if not fields or not all(col in fields for col in expected_columns):
                log(f"{os.path.basename(filepath)} missing columns. Expected: {expected_columns}, Got: {fields}", error=True, verbosity_level="ERROR")
                show_unreal_message("", "Parser error, can't proceed.", True)
                return []

            for idx, row in enumerate(reader, start=1):
                if not all(key in row for key in expected_columns):
                    log(f"[CSV] Skip malformed row {idx} in {os.path.basename(filepath)} (missing keys). Row: {row}", error=True, verbosity_level="ERROR")
                    continue
                result = row_processor(row)
                if result is not None:
                    out.append(result)
    except Exception as exc:  # noqa: BLE001
        log(f"[CSV] Error reading {os.path.basename(filepath)}: {exc}", error=True)
        return []

    return out


# ==================== Unreal Engine Assets & Actors =======================

def load_asset(asset_path: str, unreal_class: Optional[Any] = None) -> Any:
    """
    Load an asset by path with optional class cast & caching.
    Logs detailed errors on load/cast failures.
    """
    key = asset_path
    cached = _asset_cache.get(key)
    if cached is not None:
        if unreal_class is None or (cached and unreal_class.cast(cached)):
            return cached
        log(f'[Asset] Cached asset "{key}" found but not compatible with {unreal_class.__name__}, reloading.', verbosity_level='WARNING')
        _asset_cache.pop(key, None)

    loaded = editor_asset_lib.load_asset(u_path.get_base_filename(key, remove_path=False))
    if loaded is None:
        log(f'[Asset] Failed to load asset: {key}', error=True, verbosity_level='ERROR')
        return None

    if unreal_class is None:
        _asset_cache[key] = loaded
        return loaded

    try:
        casted = unreal_class.cast(loaded)
        if casted:
            _asset_cache[key] = casted
            return casted
        log(f'[Asset] Failed to cast asset "{key}" to class {unreal_class.__name__}. Asset loaded but type mismatch.', error=True, verbosity_level='ERROR')
        _asset_cache[key] = loaded
        return loaded
    except AttributeError as e:
        log(f'[Asset] AttributeError casting asset "{key}" to class {unreal_class.__name__}. Error: {str(e)}', error=True, verbosity_level='ERROR')
        _asset_cache[key] = loaded
        return loaded
    except Exception as e:
        log(f'[Asset] Unexpected error while casting asset "{key}" to class {unreal_class.__name__}. Error: {str(e)}', error=True, verbosity_level='ERROR')
        _asset_cache[key] = loaded
        return loaded


def reset_assets_cache():
    _asset_cache.clear()


def create_data_table(dt_name: str, destination_path: str, dt_struct: unreal.Struct) -> unreal.DataTable:
    """Create and save a `unreal.DataTable` asset with the given struct."""
    dt_factory = unreal.DataTableFactory()
    dt_factory.struct = dt_struct
    dt_factory.create_new = True
    dt = asset_tools.create_asset(dt_name, destination_path, unreal.DataTable.static_class(), dt_factory)
    dt_obj = unreal.DataTable.cast(dt)
    log(f'[DataTable] Created data table asset: "{dt_name}" at "{destination_path}"', verbosity_level='INFO')
    editor_asset_lib.save_loaded_asset(dt_obj)

    return dt_obj


def read_data_table(data_table: unreal.DataTable) -> List[Dict[str, Any]]:
    """Return rows of a `DataTable` as Python objects via JSON export."""
    data_table_string = data_table.export_to_json_string()
    return json.loads(data_table_string) if data_table_string else []


def spawn_actor_from_class(unreal_class, loc, rot):
    """Spawn an actor given its `unreal_class`, logging the action."""
    log(f'[Spawn] Spawned actor: {unreal_class}')
    return unreal_class.cast(level_lib.spawn_actor_from_class(unreal_class, loc, rot))


def spawn_actor_from_object(ue_object, loc, rot):
    """Spawn an actor using an asset/object reference, logging the action."""
    log(f'[Spawn] Spawned actor: {u_path.get_base_filename(ue_object.get_path_name(), remove_path=False)}')
    return level_lib.spawn_actor_from_object(ue_object, loc, rot)


def spawn_actor_from_path(ue_path, loc, rot):
    """Spawn an actor using object path, logging the action."""
    asset = load_asset(u_path.get_base_filename(ue_path.get_path_name(), remove_path=False))
    return spawn_actor_from_object(asset, loc, rot)


def get_component_by_class(unreal_actor: unreal.Actor, unreal_class: Any) -> Optional[unreal.ActorComponent]:
    """Retrieves the component of the specified class from the Unreal actor and casts it to the desired class."""
    component = unreal_actor.get_component_by_class(unreal_class)
    if not component:
        log(f"[Actor] No component of class {unreal_class.__name__} found in the actor.", error=True, verbosity_level='ERROR')
        return None

    try:
        casted_component = unreal_class.cast(component)
        if not casted_component:
            log(f"[Actor] Failed to cast component to class {unreal_class.__name__}.", error=True, verbosity_level='ERROR')
            return None
        return casted_component

    except AttributeError as e:
        log(f'[Actor] AttributeError while casting component to class {unreal_class.__name__}. Error: {str(e)}', error=True, verbosity_level='ERROR')
        return None
    except Exception as e:
        log(f'[Actor] Unexpected error while casting component to class {unreal_class.__name__}. Error: {str(e)}', error=True, verbosity_level='ERROR')
        return None


# ============================== Subprocess and S2 files =================================

def subprocess_run(args: list, show_console=False) -> Optional[subprocess.CompletedProcess]:
    """Wrapper around `subprocess.run`."""
    if show_console:
        subprocess.run(args)
        return None
    else:
        try:
            return subprocess.run(args, capture_output=True, text=True, creationflags=subprocess.CREATE_NO_WINDOW)
        except Exception as e:
            log(f'[Subprocess] Command: {args}')
            log(f'[Subprocess] An unexpected error occurred while running subprocess. Error: {str(e)}', error=True)
            raise e


def decompile_s2_batch(decompiler_exe: str, sources: Iterable[str], allowed_ext: Optional[List[str]] = None) -> None:
    """
    Run a Source 2 decompiler once over a staged temp folder, then move results back.
    - decompiler_exe: path to external decompiler
    - sources: files and/or directories to search
    - allowed_ext: e.g. [".vmat_c", ".vmdl_c", ".vtex_c"]; if None, process *all* *_c files
    """

    def _sha16(s: str) -> str:
        return hashlib.sha1(s.encode("utf-8")).hexdigest()[:16]

    def _dst_for(src: pathlib.Path) -> pathlib.Path:
        # Strip trailing "_c" from the last extension (".vmat_c" -> ".vmat")
        ext = src.suffix
        return src.with_suffix(ext[:-2]) if ext.endswith("_c") else src

    def _needs(src: pathlib.Path, dst: pathlib.Path) -> bool:
        try:
            return (not dst.exists()) or (src.stat().st_mtime > dst.stat().st_mtime)
        except FileNotFoundError:
            return True

    def _try_link(src: pathlib.Path, dst: pathlib.Path) -> bool:
        try:
            os.link(src, dst);
            return True
        except Exception:
            return False

    def _expected_temp_out(temp_in: pathlib.Path) -> pathlib.Path:
        return temp_in.with_suffix(temp_in.suffix[:-2])  # drop "_c"

    def _find_sources(items: Iterable[str], exts: Optional[List[str]]) -> List[pathlib.Path]:
        if not items:
            return []

        out: List[pathlib.Path] = []
        exts_l = [e.lower() for e in exts] if exts else None
        for it in items:
            p = pathlib.Path(it)
            if p.is_file():
                if p.suffix.lower().endswith("_c") and (not exts_l or p.suffix.lower() in exts_l):
                    out.append(p)
            elif p.is_dir():
                for root, _, files in os.walk(p):
                    for name in files:
                        q = pathlib.Path(root, name)
                        sfx = q.suffix.lower()
                        if sfx.endswith("_c") and (not exts_l or sfx in exts_l):
                            out.append(q)
        return out

    def _stage(files: List[pathlib.Path]) -> tuple[str, Dict[str, pathlib.Path], List[pathlib.Path]]:
        tmp = tempfile.mkdtemp(prefix="s2c_")
        m: Dict[str, pathlib.Path] = {}
        staged: List[pathlib.Path] = []
        root = pathlib.Path(tmp)
        for f in files:
            dst = _dst_for(f)
            if not _needs(f, dst):
                continue
            tin = root / f"{_sha16(str(f))}__{f.name}"
            tin.parent.mkdir(parents=True, exist_ok=True)
            if not _try_link(f, tin):
                shutil.copy2(f, tin)
            m[str(tin)] = dst
            staged.append(tin)
        return tmp, m, staged

    def _run_tool(exe: str, input_dir: str) -> int:
        return subprocess.run(
            [exe, "-i", input_dir, "-o", input_dir, "-d"],
            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
        ).returncode

    def _collect(temp_map: Dict[str, pathlib.Path]) -> int:
        moved = 0
        for tin_s, out_path in temp_map.items():
            tout = _expected_temp_out(pathlib.Path(tin_s))
            if tout.exists():
                out_path.parent.mkdir(parents=True, exist_ok=True)
                try:
                    shutil.move(str(tout), str(out_path))
                except Exception:
                    shutil.copy2(tout, out_path)
                    try:
                        tout.unlink()
                    except Exception:
                        pass
                moved += 1
        return moved

    candidates = _find_sources(sources, allowed_ext)
    if not candidates:
        return

    tmp, temp_map, staged = _stage(candidates)
    try:
        print(f"Found {len(candidates)}; staged {len(staged)} → {tmp}")
        if not staged:
            print("Up to date.");
            return
        rc = _run_tool(decompiler_exe, tmp)
        if rc != 0:
            print(f"Tool exit code {rc}; collecting partial results.")
        done = _collect(temp_map)
        print(f"Wrote {done} file(s); {len(staged) - done} failed.")
    finally:
        try:
            shutil.rmtree(tmp)
        except Exception:
            pass


# # ============================== Valve assets =================================

# --- regexes ------------------------------------------------------------------

RE_KEYVAL = re.compile(r'^\s*"([^"]+)"\s*"([^"]*)"\s*$')
RE_BLOCK_START = re.compile(r'^\s*"([^"]+)"\s*\{\s*$')
RE_BLOCK_END = re.compile(r'^\s*\}\s*$')
RE_VECTOR = re.compile(r'^\s*\[([^\]]+)\]\s*$')
RE_HEADER_LINE = re.compile(r'^\s*"[^"]+"\s*$')  # e.g., "Layer0"


# --- scalar coercion ----------------------------------------------------------

def _parse_vector(s: str) -> Optional[List[float]]:
    m = RE_VECTOR.match(s)
    if not m:
        return None
    parts = m.group(1).split()
    try:
        return [float(p) for p in parts]
    except ValueError:
        return None


def _coerce_value(raw: str) -> Any:
    """
    Coerce VMAT scalar value:
    - "[a b c d]" -> list[float]
    - numeric strings -> int/float
    - else -> original string
    """
    vec = _parse_vector(raw)
    if vec is not None:
        return vec
    return num(raw)


# --- VMAT reader --------------------------------------------------------------
def vmat_reader(vmat_filepath: str) -> Dict[str, Any]:
    """Parse a VMAT (KeyValues-like) file into a nested dict"""
    with open(vmat_filepath, encoding="utf-8") as f:
        lines = [ln.rstrip("\n") for ln in f]

    if not lines:
        return {}

    # Skip empty lines, then optional "LayerX" header
    idx = 0
    while idx < len(lines) and not lines[idx].strip():
        idx += 1
    if idx < len(lines) and RE_HEADER_LINE.match(lines[idx]):
        idx += 1

    root: Dict[str, Any] = {}
    stack: List[Dict[str, Any]] = [root]

    for line in lines[idx:]:
        if not line.strip():
            continue

        # Block end
        if RE_BLOCK_END.match(line):
            if len(stack) == 1:
                # Unbalanced '}' – ignore gracefully
                continue
            stack.pop()
            continue

        # Block start: "Key" {
        m = RE_BLOCK_START.match(line)
        if m:
            key = m.group(1)
            new_block: Dict[str, Any] = {}
            stack[-1][key] = new_block
            stack.append(new_block)
            continue

        # Key/Value: "Key" "Value"
        m = RE_KEYVAL.match(line)
        if m:
            key, raw_val = m.group(1), m.group(2)
            stack[-1][key] = _coerce_value(raw_val)
            continue

    # Remove top-level Texture* keys
    root = {k: v for k, v in root.items() if not k.startswith("Texture")}
    return root


# --- high-level parser --------------------------------------------------------

def vmat_parser(material_path: str, only_allowed_shader: bool = False,
                shadername: str = ".vfx") -> Optional[Dict[str, Any]]:
    """Parse a VMAT into normalized parameters."""
    material: Dict[str, Any] = vmat_reader(material_path)

    if only_allowed_shader and material.get('shader') != f'{shadername}.vfx':
        return None

    out: Dict[str, Any] = {}

    prefix_casters = (
        (("g_b", "F_"), as_bool),
        (("g_n", "g_fl"), as_float),
        (("g_v",), as_vec),
    )

    for key, val in material.items():
        # Skip sticker-related top-level keys (unchanged from your code)
        if "sticker" in key.lower():
            continue

        # 1) If this is a nested block, copy/normalize its inner values
        if isinstance(val, dict):
            for nkey, nval in val.items():
                # Try RGBA coercion for vector-like leaves
                try:
                    if (
                            isinstance(nval, (list, tuple)) and len(nval) == 4
                    ) or (isinstance(nval, str) and RE_VECTOR.match(nval)):
                        nval = as_vec(nval)
                except Exception:
                    pass
                out[nkey] = nval

            # For blocks, also keep the original block under its key
            out[key] = val
            continue

        # 2) Scalar or list/tuple value at the top level — cast by prefix
        casted = val
        for prefixes, caster in prefix_casters:
            if key.startswith(prefixes):
                try:
                    casted = caster(val)
                except Exception:
                    log(f"WARN: Failed to cast {key}={val!r} with {caster.__name__}")
                    pass
                break

        # Also normalize vector-like scalars at top level
        try:
            if (
                    isinstance(casted, (list, tuple)) and len(casted) == 4
            ) or (isinstance(casted, str) and RE_VECTOR.match(casted)):
                casted = as_vec(casted)
        except Exception:
            pass

        out[key] = casted

    return out


def kv3_parser(path: str):
    """
    Parse a Valve KV3 file (e.g., .vcompmat) into Python structures.

    Args:
        path: Path to the .vcompmat/.kv3 file.

    Returns:
        dict / list / str / int / float / bool / None.
    """

    TokenKind = Literal[
        "WS", "COMMENT",
        "LBRACE", "RBRACE", "LBRACK", "RBRACK",
        "EQUAL", "COLON", "COMMA",
        "STRING", "NUMBER", "IDENT",
        "OTHER",
        "EOF",
    ]
    Token = Tuple[TokenKind, str, int]

    with open(path, "r", encoding="utf-8", errors="replace") as f:
        text = f.read()

    token_spec = [
        ("WS", r"[ \t\r\n]+"),
        ("COMMENT", r"//[^\n]*"),
        ("LBRACE", r"\{"),
        ("RBRACE", r"\}"),
        ("LBRACK", r"\["),
        ("RBRACK", r"\]"),
        ("EQUAL", r"="),
        ("COLON", r":"),
        ("COMMA", r","),
        ("STRING", r'"(?:\\.|[^"\\])*"'),
        ("NUMBER", r"-?(?:0|[1-9]\d*)(?:\.\d+)?(?:[eE][+-]?\d+)?"),
        ("IDENT", r"[A-Za-z_][A-Za-z0-9_]*"),
        ("OTHER", r"."),
    ]
    tok_re = re.compile("|".join(f"(?P<{n}>{p})" for n, p in token_spec), re.S)

    def _unescape(literal: str) -> str:
        body = literal[1:-1]
        return bytes(body, "utf-8").decode("unicode_escape")

    def _tokenize(source: str) -> Iterator[Token]:
        # strip KV3 XML header comment <!-- ... -->
        source = re.sub(r"<!--.*?-->\s*", "", source, flags=re.S)
        for m in tok_re.finditer(source):
            kind = m.lastgroup
            # Narrow None away for type checkers
            assert kind is not None
            val = m.group()
            if kind in ("WS", "COMMENT"):
                continue
            if kind == "STRING":
                val = _unescape(val)
            yield (kind, val, m.start())
        # Single EOF token to simplify peek logic
        yield "EOF", "", len(source)

    tokens = list(_tokenize(text))
    index = 0

    def _peek() -> Token:
        return tokens[index]

    def _pop(expected: TokenKind | None = None) -> Token:
        nonlocal index
        tok = _peek()
        if expected is not None and tok[0] != expected:
            raise SyntaxError(f"Expected {expected}, got {tok[0]} at char {tok[2]}")
        index += 1
        return tok

    def _next_is(kind: TokenKind) -> bool:
        return index + 1 < len(tokens) and tokens[index + 1][0] == kind

    def _parse_value() -> Any:
        kind, val, pos = _peek()

        if kind == "LBRACE":
            return _parse_object()
        if kind == "LBRACK":
            return _parse_array()
        if kind == "STRING":
            _pop()
            return val
        if kind == "NUMBER":
            _pop()
            if any(c in val for c in ".eE"):
                try:
                    return float(val)
                except ValueError:
                    return val
            try:
                return int(val)
            except ValueError:
                return val
        if kind == "IDENT":
            # typed value: IDENT ':' <value>
            if _next_is("COLON"):
                type_name = val
                _pop()  # IDENT
                _pop("COLON")  # :
                inner = _parse_value()
                return inner

            # literals
            if val == "true":
                _pop()
                return True
            if val == "false":
                _pop()
                return False
            if val == "null":
                _pop()
                return None
            if val in ("Infinity", "-Infinity", "NaN"):
                _pop()
                return None

            # bare identifier -> string
            _pop()
            return val

        raise SyntaxError(f"Unexpected token {kind} {val!r} at char {pos}")

    def _parse_key() -> str:
        kind, val, pos = _peek()
        if kind in ("IDENT", "STRING"):
            _pop()
            return val
        raise SyntaxError(f"Expected key, got {kind} at char {pos}")

    def _parse_object() -> Dict[str, Any]:
        _pop("LBRACE")
        obj: Dict[str, Any] = {}
        first = True
        while True:
            kind, _, _ = _peek()
            if kind == "RBRACE":
                _pop()
                break
            if not first and kind == "COMMA":
                _pop()
                kind, _, _ = _peek()
            if kind == "RBRACE":
                _pop()
                break

            key = _parse_key()
            sep_kind, _, pos = _peek()
            if sep_kind not in ("EQUAL", "COLON"):
                raise SyntaxError(f'Expected "=" or ":", got {sep_kind} at char {pos}')
            _pop()  # '=' or ':'
            obj[key] = _parse_value()
            first = False
        return obj

    def _parse_array() -> list[Any]:
        _pop("LBRACK")
        arr: list[Any] = []
        first = True
        while True:
            kind, _, _ = _peek()
            if kind == "RBRACK":
                _pop()
                break
            if not first and kind == "COMMA":
                _pop()
                kind, _, _ = _peek()
            if kind == "RBRACK":
                _pop()
                break
            arr.append(_parse_value())
            first = False
        return arr

    result = _parse_value()

    # After a valid parse, only EOF should remain
    kind, _, pos = _peek()
    if kind != "EOF":
        raise SyntaxError(f"Trailing content after valid KV3 at char {pos}: {kind}")

    return result
